{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Installations and imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:00:28.294089Z","iopub.status.busy":"2024-08-11T11:00:28.293727Z","iopub.status.idle":"2024-08-11T11:02:40.912325Z","shell.execute_reply":"2024-08-11T11:02:40.911108Z","shell.execute_reply.started":"2024-08-11T11:00:28.294059Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.2 which is incompatible.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.17.0 which is incompatible.\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q -U \"torch==2.1.2\" tensorboard"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:02:40.914735Z","iopub.status.busy":"2024-08-11T11:02:40.914414Z","iopub.status.idle":"2024-08-11T11:03:15.886644Z","shell.execute_reply":"2024-08-11T11:03:15.885287Z","shell.execute_reply.started":"2024-08-11T11:02:40.914707Z"},"trusted":true},"outputs":[],"source":["!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\""]},{"cell_type":"markdown","metadata":{},"source":["The code imports the os module and sets two environment variables:\n","* CUDA_VISIBLE_DEVICES: This environment variable tells PyTorch which GPUs to use. In this case, the code is setting the environment variable to 0, which means that PyTorch will use the first GPU.\n","* TOKENIZERS_PARALLELISM: This environment variable tells the Hugging Face Transformers library whether to parallelize the tokenization process. In this case, the code is setting the environment variable to false, which means that the tokenization process will not be parallelized."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:03:15.888334Z","iopub.status.busy":"2024-08-11T11:03:15.888051Z","iopub.status.idle":"2024-08-11T11:04:24.329539Z","shell.execute_reply":"2024-08-11T11:04:24.328168Z","shell.execute_reply.started":"2024-08-11T11:03:15.888306Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n","chex 0.1.82 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\n","tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.17.0 which is incompatible.\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\n","ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n","!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:04:24.332441Z","iopub.status.busy":"2024-08-11T11:04:24.332095Z","iopub.status.idle":"2024-08-11T11:04:24.337306Z","shell.execute_reply":"2024-08-11T11:04:24.336476Z","shell.execute_reply.started":"2024-08-11T11:04:24.332413Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:04:24.339557Z","iopub.status.busy":"2024-08-11T11:04:24.339031Z","iopub.status.idle":"2024-08-11T11:04:24.355066Z","shell.execute_reply":"2024-08-11T11:04:24.354171Z","shell.execute_reply.started":"2024-08-11T11:04:24.339525Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:04:24.356410Z","iopub.status.busy":"2024-08-11T11:04:24.356169Z","iopub.status.idle":"2024-08-11T11:04:41.520912Z","shell.execute_reply":"2024-08-11T11:04:41.520018Z","shell.execute_reply.started":"2024-08-11T11:04:24.356381Z"},"papermill":{"duration":14.485002,"end_time":"2023-10-16T11:00:18.917449","exception":false,"start_time":"2023-10-16T11:00:04.432447","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from datasets import Dataset\n","from peft import LoraConfig, PeftConfig\n","from trl import SFTTrainer\n","from trl import setup_chat_format\n","from transformers import (AutoModelForCausalLM, \n","                          AutoTokenizer, \n","                          BitsAndBytesConfig, \n","                          TrainingArguments, \n","                          pipeline, \n","                          logging)\n","from sklearn.metrics import (accuracy_score, \n","                             classification_report, \n","                             confusion_matrix)\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:04:41.522313Z","iopub.status.busy":"2024-08-11T11:04:41.522026Z","iopub.status.idle":"2024-08-11T11:04:41.528931Z","shell.execute_reply":"2024-08-11T11:04:41.527221Z","shell.execute_reply.started":"2024-08-11T11:04:41.522289Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["pytorch version 2.1.2+cu121\n"]}],"source":["print(f\"pytorch version {torch.__version__}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:04:41.531001Z","iopub.status.busy":"2024-08-11T11:04:41.530701Z","iopub.status.idle":"2024-08-11T11:04:41.545317Z","shell.execute_reply":"2024-08-11T11:04:41.544349Z","shell.execute_reply.started":"2024-08-11T11:04:41.530975Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["working on cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"working on {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing the data and the core evaluation functions"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:04:41.547081Z","iopub.status.busy":"2024-08-11T11:04:41.546688Z","iopub.status.idle":"2024-08-11T11:04:42.886056Z","shell.execute_reply":"2024-08-11T11:04:42.885058Z","shell.execute_reply.started":"2024-08-11T11:04:41.547048Z"},"trusted":true},"outputs":[],"source":["filename = \"../input/sentiment-analysis-for-financial-news/all-data.csv\"\n","\n","df = pd.read_csv(filename, \n","                 names=[\"sentiment\", \"text\"],\n","                 encoding=\"utf-8\", encoding_errors=\"replace\")\n","\n","X_train = list()\n","X_test = list()\n","for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n","    train, test  = train_test_split(df[df.sentiment==sentiment], \n","                                    train_size=300,\n","                                    test_size=300, \n","                                    random_state=42)\n","    X_train.append(train)\n","    X_test.append(test)\n","\n","X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n","X_test = pd.concat(X_test)\n","\n","eval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\n","X_eval = df[df.index.isin(eval_idx)]\n","X_eval = (X_eval\n","          .groupby('sentiment', group_keys=False)\n","          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n","X_train = X_train.reset_index(drop=True)\n","\n","def generate_prompt(data_point):\n","    return f\"\"\"\n","            Analyze the sentiment of the news headline enclosed in square brackets, \n","            determine if it is positive, neutral, or negative, and return the answer as \n","            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n","\n","            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n","            \"\"\".strip()\n","\n","def generate_test_prompt(data_point):\n","    return f\"\"\"\n","            Analyze the sentiment of the news headline enclosed in square brackets, \n","            determine if it is positive, neutral, or negative, and return the answer as \n","            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n","\n","            [{data_point[\"text\"]}] = \"\"\".strip()\n","\n","X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n","                       columns=[\"text\"])\n","X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n","                      columns=[\"text\"])\n","\n","y_true = X_test.sentiment\n","X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n","\n","train_data = Dataset.from_pandas(X_train)\n","eval_data = Dataset.from_pandas(X_eval)"]},{"cell_type":"markdown","metadata":{},"source":["# Function to evaluate the result"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:04:42.890794Z","iopub.status.busy":"2024-08-11T11:04:42.890483Z","iopub.status.idle":"2024-08-11T11:04:42.900301Z","shell.execute_reply":"2024-08-11T11:04:42.899221Z","shell.execute_reply.started":"2024-08-11T11:04:42.890768Z"},"trusted":true},"outputs":[],"source":["def evaluate(y_true, y_pred):\n","    labels = ['positive', 'neutral', 'negative']\n","    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n","    def map_func(x):\n","        return mapping.get(x, 1)\n","    \n","    y_true = np.vectorize(map_func)(y_true)\n","    y_pred = np.vectorize(map_func)(y_pred)\n","    \n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n","    print(f'Accuracy: {accuracy:.3f}')\n","    \n","    # Generate accuracy report\n","    unique_labels = set(y_true)  # Get unique labels\n","    \n","    for label in unique_labels:\n","        label_indices = [i for i in range(len(y_true)) \n","                         if y_true[i] == label]\n","        label_y_true = [y_true[i] for i in label_indices]\n","        label_y_pred = [y_pred[i] for i in label_indices]\n","        accuracy = accuracy_score(label_y_true, label_y_pred)\n","        print(f'Accuracy for label {label}: {accuracy:.3f}')\n","        \n","    # Generate classification report\n","    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n","    print('\\nClassification Report:')\n","    print(class_report)\n","    \n","    # Generate confusion matrix\n","    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n","    print('\\nConfusion Matrix:')\n","    print(conf_matrix)"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the model without fine-tuning"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:04:42.902346Z","iopub.status.busy":"2024-08-11T11:04:42.901644Z","iopub.status.idle":"2024-08-11T11:06:54.334909Z","shell.execute_reply":"2024-08-11T11:06:54.333870Z","shell.execute_reply.started":"2024-08-11T11:04:42.902319Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b1f4ffee7c348e69116411aa0acf77e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name=\"/kaggle/input/llama-2/pytorch/7b-hf/1\"\n","# Download the model from Hugging Face Hub\n","compute_dtype = getattr(torch, \"float16\")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, \n","    bnb_4bit_quant_type=\"nf4\", \n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=device,\n","    torch_dtype=compute_dtype,\n","    quantization_config=bnb_config, \n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, \n","                                          trust_remote_code=True,\n","                                         )\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","model, tokenizer = setup_chat_format(model, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["# Predict function"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:06:54.336628Z","iopub.status.busy":"2024-08-11T11:06:54.336239Z","iopub.status.idle":"2024-08-11T11:06:54.344646Z","shell.execute_reply":"2024-08-11T11:06:54.343645Z","shell.execute_reply.started":"2024-08-11T11:06:54.336596Z"},"trusted":true},"outputs":[],"source":["def predict(test, model, tokenizer):\n","    y_pred = []\n","    for i in tqdm(range(len(X_test))):\n","        prompt = X_test.iloc[i][\"text\"]\n","        pipe = pipeline(task=\"text-generation\", \n","                        model=model, \n","                        tokenizer=tokenizer, \n","                        max_new_tokens = 1, \n","                        temperature = 0.0,\n","                       )\n","        result = pipe(prompt)\n","        answer = result[0]['generated_text'].split(\"=\")[-1]\n","        if \"positive\" in answer:\n","            y_pred.append(\"positive\")\n","        elif \"negative\" in answer:\n","            y_pred.append(\"negative\")\n","        elif \"neutral\" in answer:\n","            y_pred.append(\"neutral\")\n","        else:\n","            y_pred.append(\"none\")\n","    return y_pred"]},{"cell_type":"markdown","metadata":{},"source":["At this point, we are ready to test the Llama 2 7b-hf model and see how it performs on our problem without any fine-tuning. This allows us to get insights on the model itself and establish a baseline."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:06:54.346125Z","iopub.status.busy":"2024-08-11T11:06:54.345845Z","iopub.status.idle":"2024-08-11T11:12:22.507058Z","shell.execute_reply":"2024-08-11T11:12:22.506040Z","shell.execute_reply.started":"2024-08-11T11:06:54.346101Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 900/900 [05:28<00:00,  2.74it/s]\n"]}],"source":["y_pred = predict(test, model, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["In the following cell, we evaluate the results. There is little to be said, it is performing really terribly because the 7b-hf model tends to just predict a neutral sentiment and seldom it detects positive or negative sentiment. In future, Will explore more sophisticated model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:12:22.508766Z","iopub.status.busy":"2024-08-11T11:12:22.508427Z","iopub.status.idle":"2024-08-11T11:12:22.532563Z","shell.execute_reply":"2024-08-11T11:12:22.531499Z","shell.execute_reply.started":"2024-08-11T11:12:22.508739Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.373\n","Accuracy for label 0: 0.027\n","Accuracy for label 1: 0.937\n","Accuracy for label 2: 0.157\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.03      0.05       300\n","           1       0.34      0.94      0.50       300\n","           2       0.67      0.16      0.25       300\n","\n","    accuracy                           0.37       900\n","   macro avg       0.63      0.37      0.27       900\n","weighted avg       0.63      0.37      0.27       900\n","\n","\n","Confusion Matrix:\n","[[  8 287   5]\n"," [  1 281  18]\n"," [  0 253  47]]\n"]}],"source":["evaluate(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["## Fine-tuning"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:12:22.534340Z","iopub.status.busy":"2024-08-11T11:12:22.533969Z","iopub.status.idle":"2024-08-11T11:12:25.483970Z","shell.execute_reply":"2024-08-11T11:12:25.483129Z","shell.execute_reply.started":"2024-08-11T11:12:22.534305Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9323eba86b244cb9ceef6ff27e57c71","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/900 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2472f9e5ba4c4e24a09a1ea699c4d85b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/150 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["output_dir=\"trained_weigths\"\n","\n","peft_config = LoraConfig(\n","        lora_alpha=16, \n","        lora_dropout=0.1,\n","        r=64,\n","        bias=\"none\",\n","        target_modules=\"all-linear\",\n","        task_type=\"CAUSAL_LM\",\n",")\n","\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,                    \n","    num_train_epochs=3,                       \n","    per_device_train_batch_size=1,            \n","    gradient_accumulation_steps=8,            \n","    gradient_checkpointing=True,             \n","    optim=\"paged_adamw_32bit\",\n","    save_steps=0,\n","    logging_steps=25,                       \n","    learning_rate=2e-4,                      \n","    weight_decay=0.001,\n","    fp16=True,\n","    bf16=False,\n","    max_grad_norm=0.3,                     \n","    max_steps=-1,\n","    warmup_ratio=0.03,                        \n","    group_by_length=True,\n","    lr_scheduler_type=\"cosine\",            \n","    report_to=\"tensorboard\",                \n","    evaluation_strategy=\"epoch\"               \n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=training_arguments,\n","    train_dataset=train_data,\n","    eval_dataset=eval_data,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    max_seq_length=1024,\n","    packing=False,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,\n","        \"append_concat_token\": False,\n","    }\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# train and save model"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T11:12:25.485790Z","iopub.status.busy":"2024-08-11T11:12:25.485476Z","iopub.status.idle":"2024-08-11T12:13:38.992444Z","shell.execute_reply":"2024-08-11T12:13:38.991504Z","shell.execute_reply.started":"2024-08-11T11:12:25.485765Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [336/336 1:00:59, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.802500</td>\n","      <td>0.700173</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.516900</td>\n","      <td>0.714190</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=336, training_loss=0.717708055462156, metrics={'train_runtime': 3672.9081, 'train_samples_per_second': 0.735, 'train_steps_per_second': 0.091, 'total_flos': 1.0717884041527296e+16, 'train_loss': 0.717708055462156, 'epoch': 2.99})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Train model\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["The model and the tokenizer are saved to disk for later usage."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T12:13:38.993959Z","iopub.status.busy":"2024-08-11T12:13:38.993621Z","iopub.status.idle":"2024-08-11T12:13:43.115139Z","shell.execute_reply":"2024-08-11T12:13:43.114332Z","shell.execute_reply.started":"2024-08-11T12:13:38.993931Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('trained_weigths/tokenizer_config.json',\n"," 'trained_weigths/special_tokens_map.json',\n"," 'trained_weigths/tokenizer.model',\n"," 'trained_weigths/added_tokens.json',\n"," 'trained_weigths/tokenizer.json')"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Save trained model and tokenizer\n","trainer.save_model()\n","tokenizer.save_pretrained(output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["## Saving model to disk for later usage"]},{"cell_type":"markdown","metadata":{},"source":["Before proceeding, we first remove the previous model and clean up the memory from various objects we won't use anymore."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T12:13:50.153385Z","iopub.status.busy":"2024-08-11T12:13:50.153104Z","iopub.status.idle":"2024-08-11T12:13:50.160545Z","shell.execute_reply":"2024-08-11T12:13:50.159488Z","shell.execute_reply.started":"2024-08-11T12:13:50.153354Z"},"trusted":true},"outputs":[],"source":["import gc\n","\n","del [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\n","del [df, X_train, X_eval]\n","del [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T12:13:50.162409Z","iopub.status.busy":"2024-08-11T12:13:50.161811Z","iopub.status.idle":"2024-08-11T12:14:21.908013Z","shell.execute_reply":"2024-08-11T12:14:21.907213Z","shell.execute_reply.started":"2024-08-11T12:13:50.162373Z"},"trusted":true},"outputs":[],"source":["for _ in range(100):\n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T12:14:21.911329Z","iopub.status.busy":"2024-08-11T12:14:21.910943Z","iopub.status.idle":"2024-08-11T12:14:23.007677Z","shell.execute_reply":"2024-08-11T12:14:23.006478Z","shell.execute_reply.started":"2024-08-11T12:14:21.911294Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun Aug 11 12:14:22 2024       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   58C    P0             41W /  250W |    1927MiB /  16384MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["Then we can proceed to merging the weights and we will be using the merged model for our testing purposes."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T12:14:23.009617Z","iopub.status.busy":"2024-08-11T12:14:23.009272Z","iopub.status.idle":"2024-08-11T12:15:28.030177Z","shell.execute_reply":"2024-08-11T12:15:28.028723Z","shell.execute_reply.started":"2024-08-11T12:14:23.009589Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0538a3a4b6647ee8720c5a12d54cbab","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["('./merged_model/tokenizer_config.json',\n"," './merged_model/special_tokens_map.json',\n"," './merged_model/tokenizer.model',\n"," './merged_model/added_tokens.json',\n"," './merged_model/tokenizer.json')"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["from peft import AutoPeftModelForCausalLM\n","\n","finetuned_model = \"./trained_weigths/\"\n","compute_dtype = getattr(torch, \"float16\")\n","tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-2/pytorch/7b-hf/1\")\n","\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","     finetuned_model,\n","     torch_dtype=compute_dtype,\n","     return_dict=False,\n","     low_cpu_mem_usage=True,\n","     device_map=device,\n",")\n","\n","merged_model = model.merge_and_unload()\n","merged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\n","tokenizer.save_pretrained(\"./merged_model\")"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"markdown","metadata":{},"source":["The following code will first predict the sentiment labels for the test set using the predict() function. Then, it will evaluate the model's performance on the test set using the evaluate() function. The result now should be impressive with an overall accuracy of over 0.8 and high accuracy, precision and recall for the single sentiment labels. The prediction of the neutral label can still be improved, yet it is impressive how much could be done with little data and some fine-tuning."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T12:15:28.034250Z","iopub.status.busy":"2024-08-11T12:15:28.032731Z","iopub.status.idle":"2024-08-11T12:19:18.167569Z","shell.execute_reply":"2024-08-11T12:19:18.166573Z","shell.execute_reply.started":"2024-08-11T12:15:28.034202Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 900/900 [03:50<00:00,  3.91it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.852\n","Accuracy for label 0: 0.900\n","Accuracy for label 1: 0.857\n","Accuracy for label 2: 0.800\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.90      0.93       300\n","           1       0.75      0.86      0.80       300\n","           2       0.87      0.80      0.83       300\n","\n","    accuracy                           0.85       900\n","   macro avg       0.86      0.85      0.85       900\n","weighted avg       0.86      0.85      0.85       900\n","\n","\n","Confusion Matrix:\n","[[270  27   3]\n"," [ 10 257  33]\n"," [  3  57 240]]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["y_pred = predict(test, merged_model, tokenizer)\n","evaluate(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["The following code will create a Pandas DataFrame called evaluation containing the text, true labels, and predicted labels from the test set. This is expectially useful for understanding the errors that the fine-tuned model makes, and gettting insights on how to improve the prompt."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T12:19:18.169264Z","iopub.status.busy":"2024-08-11T12:19:18.168897Z","iopub.status.idle":"2024-08-11T12:19:18.205642Z","shell.execute_reply":"2024-08-11T12:19:18.204725Z","shell.execute_reply.started":"2024-08-11T12:19:18.169230Z"},"trusted":true},"outputs":[],"source":["evaluation = pd.DataFrame({'text': X_test[\"text\"], \n","                           'y_true':y_true, \n","                           'y_pred': y_pred},\n","                         )\n","evaluation.to_csv(\"test_predictions.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["Here are the results of the baseline model:\n","\n","Accuracy: 0.623\n","Accuracy for label 0: 0.620\n","Accuracy for label 1: 0.590\n","Accuracy for label 2: 0.660\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.62      0.69       300\n","           1       0.61      0.59      0.60       300\n","           2       0.53      0.66      0.59       300\n","\n","    accuracy                           0.62       900\n","   macro avg       0.64      0.62      0.63       900\n","weighted avg       0.64      0.62      0.63       900\n","\n","\n","Confusion Matrix:\n","\n","[[186  39  75]\\\n"," [ 23 177 100]\\\n"," [ 27  75 198]]\n"," "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":622510,"sourceId":1192499,"sourceType":"datasetVersion"},{"modelId":735,"modelInstanceId":3090,"sourceId":4295,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
